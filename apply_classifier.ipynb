{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:19:17.790715Z",
     "end_time": "2023-11-22T08:19:17.798189Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "plt.rcParams['animation.ffmpeg_path'] = r\"C:\\Users\\rfgla\\Documents\\Ray\\ffmpeg-master-latest-win64-gpl\\ffmpeg-master-latest-win64-gpl\\bin\\ffmpeg.exe\"\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.io import read_video, read_video_timestamps\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\VideoGPT-master')\n",
    "from videogpt import download, VQVAE, VideoGPT\n",
    "from videogpt.data import VideoData\n",
    "from convolutional_classifier import Classifier as ConvolutionalClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Create arg parser\n",
    "vqvae_path = r\"C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\VideoGPT-master\\lightning_logs\\version_23\\checkpoints\\epoch=60-step=188489.ckpt\"\n",
    "data_path = r\"C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\data\\gesture_sorted_data\"\n",
    "config = {\n",
    "        \"--batch_size\": 1,\n",
    "        \"--vqvae\": vqvae_path,\n",
    "        \"--kernel_size\": 3,\n",
    "        \"--out_channels\": 3,\n",
    "        \"--n_classes\": 8,\n",
    "        \"--lr\": 8e-4,\n",
    "        \"--gpus\": 1,\n",
    "        \"--data_path\": data_path,\n",
    "    }\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser.add_argument('--data_path', type=str, default='/home/wilson/data/datasets/bair.hdf5')\n",
    "parser.add_argument('--sequence_length', type=int, default=16)\n",
    "parser.add_argument('--resolution', type=int, default=64)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--num_workers', type=int, default=8)\n",
    "parser = ConvolutionalClassifier.add_model_specific_args(parser)\n",
    "config_args = []\n",
    "for key in config.keys():\n",
    "        config_args.append(key)\n",
    "        config_args.append(str(config[key]))\n",
    "args = parser.parse_args(config_args)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:19:19.548567Z",
     "end_time": "2023-11-22T08:19:19.561953Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "data = VideoData(args)\n",
    "test_loader = data.test_dataloader()\n",
    "args.n_classes = data.n_classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:19:19.562999Z",
     "end_time": "2023-11-22T08:19:19.688245Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "Classifier(\n  (vqvae): VQVAE(\n    (encoder): Encoder(\n      (convs): ModuleList(\n        (0): SamePadConv3d(\n          (conv): Conv3d(3, 240, kernel_size=(4, 4, 4), stride=(2, 2, 2))\n        )\n        (1): SamePadConv3d(\n          (conv): Conv3d(240, 240, kernel_size=(4, 4, 4), stride=(2, 2, 2))\n        )\n      )\n      (conv_last): SamePadConv3d(\n        (conv): Conv3d(240, 240, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n      )\n      (res_stack): Sequential(\n        (0): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (1): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (2): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (3): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (4): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU()\n      )\n    )\n    (decoder): Decoder(\n      (res_stack): Sequential(\n        (0): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (1): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (2): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (3): AttentionResidualBlock(\n          (block): Sequential(\n            (0): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (1): ReLU()\n            (2): SamePadConv3d(\n              (conv): Conv3d(240, 120, kernel_size=(3, 3, 3), stride=(1, 1, 1), bias=False)\n            )\n            (3): BatchNorm3d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (4): ReLU()\n            (5): SamePadConv3d(\n              (conv): Conv3d(120, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n            )\n            (6): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (7): ReLU()\n            (8): AxialBlock(\n              (attn_w): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_h): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n              (attn_t): MultiHeadAttention(\n                (w_qs): Linear(in_features=240, out_features=240, bias=False)\n                (w_ks): Linear(in_features=240, out_features=240, bias=False)\n                (w_vs): Linear(in_features=240, out_features=240, bias=False)\n                (fc): Linear(in_features=240, out_features=240, bias=True)\n                (attn): AxialAttention()\n              )\n            )\n          )\n        )\n        (4): BatchNorm3d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU()\n      )\n      (convts): ModuleList(\n        (0): SamePadConvTranspose3d(\n          (convt): ConvTranspose3d(240, 240, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(3, 3, 3))\n        )\n        (1): SamePadConvTranspose3d(\n          (convt): ConvTranspose3d(240, 3, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(3, 3, 3))\n        )\n      )\n    )\n    (pre_vq_conv): SamePadConv3d(\n      (conv): Conv3d(240, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    )\n    (post_vq_conv): SamePadConv3d(\n      (conv): Conv3d(256, 240, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n    )\n    (codebook): Codebook()\n  )\n  (conv1): Conv3d(3, 3, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n  (relu1): ReLU()\n  (maxpool1): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv3d(3, 3, kernel_size=(1, 3, 3), stride=(1, 1, 1))\n  (relu2): ReLU()\n  (maxpool2): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 3, 3), padding=0, dilation=1, ceil_mode=False)\n  (FC1): Linear(in_features=1728, out_features=9, bias=True)\n  (relu3): ReLU()\n  (FC2): Linear(in_features=9, out_features=9, bias=True)\n  (logSoftmax): LogSoftmax(dim=1)\n)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load classifier from checkpoint\n",
    "classifier_path = r\"C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\convolutional_classifier\\final_model.ckpt\"\n",
    "device = torch.device('cuda')\n",
    "model = ConvolutionalClassifier(args)\n",
    "optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=args.lr)\n",
    "\n",
    "checkpoint = torch.load(classifier_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:19:19.688245Z",
     "end_time": "2023-11-22T08:19:20.124368Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "correct = 0\n",
    "count = 0\n",
    "for idx, batch in enumerate(test_loader):\n",
    "    # Send to the gpu\n",
    "    video, label = batch[\"video\"], batch[\"label\"]\n",
    "    video, label = video.to(device), label.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(video)\n",
    "        result = predictions.argmax(axis=1) == label\n",
    "\n",
    "        correct += result.sum().item()\n",
    "        count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:19:26.835581Z",
     "end_time": "2023-11-22T08:20:58.253992Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6687268232385661\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct / count\n",
    "print(accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-22T08:20:58.257013Z",
     "end_time": "2023-11-22T08:20:58.271042Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
