{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:09:20.434948Z",
     "start_time": "2023-11-22T14:09:17.836777Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "plt.rcParams['animation.ffmpeg_path'] = r\"C:\\Users\\rfgla\\Documents\\Ray\\ffmpeg-master-latest-win64-gpl\\ffmpeg-master-latest-win64-gpl\\bin\\ffmpeg.exe\"\n",
    "import torch\n",
    "from torchvision.io import read_video, read_video_timestamps\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\VideoGPT-master')\n",
    "from videogpt import download, VQVAE, VideoGPT\n",
    "from videogpt.data import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:14:21.529884Z",
     "start_time": "2023-11-22T14:14:19.704364Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load in video gpt model\n",
    "gpt_path = r\"C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\VideoGPT-master\\lightning_logs\\version_26\\checkpoints\\epoch=45-step=142139.ckpt\"\n",
    "device = torch.device('cuda')\n",
    "gpt = VideoGPT.load_from_checkpoint(gpt_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:14:21.582351Z",
     "start_time": "2023-11-22T14:14:21.529884Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4975, device='cuda:0') tensor(-0.5000, device='cuda:0')\n",
      "torch.Size([1, 3, 16, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Load in video data\n",
    "video_filename = r\"C:\\Users\\rfgla\\Documents\\Ray\\telerehab_exercise_feedback\\data\\gesture_sorted_data\\test\\EFL\\103_18_0_4_1_stand.mp4\"\n",
    "resolution, sequence_length = gpt.args.resolution, 16\n",
    "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
    "video = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
    "video = preprocess(video, resolution, sequence_length).unsqueeze(0).to(device)\n",
    "print(torch.max(video), torch.min(video))\n",
    "print(video.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Visualize the input video\n",
    "# Visualize the reconstruction\n",
    "print(video.shape)\n",
    "disp_video = video.copy()\n",
    "disp_video = disp_video[0].permute(1, 2, 3, 0)  # CTHW -> THWC\n",
    "videos = ((disp_video + 0.5) * 255).cpu().numpy().astype('uint8')\n",
    "print(videos.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "im = plt.imshow(videos[0, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(videos[0, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(videos[i, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:14:22.040051Z",
     "start_time": "2023-11-22T14:14:22.027952Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Get the label for the data\n",
    "if video_filename.find(\"gesture_sorted_data\") != -1:  # Folders will be for each separate gesture\n",
    "    classes = [\"EFL\", \"EFR\", \"SFL\", \"SFR\", \"SAL\", \"SAR\", \"SFE\", \"STL\", \"STR\"]\n",
    "else:  # Folders will be for both left and right variants of each gesture\n",
    "    classes = [\"EF\", \"SF\", \"SA\", \"SFE\", \"ST\"]\n",
    "\n",
    "for l, this_label in enumerate(classes):\n",
    "    if video_filename.find(this_label) != -1:\n",
    "        label = torch.tensor([l])\n",
    "        continue\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:15:10.235136Z",
     "start_time": "2023-11-22T14:14:22.857667Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|██████████████████████████████████████████████████████████████████████████████████                                                                                                             | 440/1024 [00:07<00:08, 69.69it/s]"
     ]
    }
   ],
   "source": [
    "label = torch.tensor([7])\n",
    "n_iterations = 3  # Video will be n_iterations * sequence_length long\n",
    "n_cond_frames = gpt.args.n_cond_frames\n",
    "samples = [[] for _ in range(n_iterations)]\n",
    "for this_iteration in range(n_iterations):\n",
    "    if this_iteration != 0:\n",
    "        video[:, :, :, :, :] = torch.flip(samples[this_iteration - 1], dims=[2])  # Update video to be last predicted frames played in reverse\n",
    "    batch = {'video': video, 'label': label}\n",
    "    samples[this_iteration] = gpt.sample(1, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:15:12.572707Z",
     "start_time": "2023-11-22T14:15:12.542095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "all_samples = torch.cat([this_sample for this_sample in samples], axis=2)\n",
    "print(all_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T14:15:18.130574Z",
     "start_time": "2023-11-22T14:15:13.587710Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "b, c, t, h, w = all_samples.shape\n",
    "disp_samples = all_samples\n",
    "print(disp_samples.shape)\n",
    "disp_samples = disp_samples.permute(0, 2, 3, 4, 1)\n",
    "disp_samples = (disp_samples.cpu().numpy() * 255).astype('uint8')\n",
    "\n",
    "videos = disp_samples[0]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "im = plt.imshow(videos[0, :, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(videos[0, :, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(videos[i, :, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
