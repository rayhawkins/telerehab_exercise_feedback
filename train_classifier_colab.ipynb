{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install most package dependencies\n",
    "!pip install -r minimal_requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Install remaining package dependencies (these ones are trickier with version control so need to be installed separately)\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install pytorch-lightning==1.8\n",
    "!pip install sk-video\n",
    "!pip install av"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('/content/drive/BME1570/')\n",
    "sys.path.append('/content/drive/BME1570/VideoGPT-master')\n",
    "from tunable_data import VideoData\n",
    "from transformer_classifier import Classifier as TransformerClassifier\n",
    "from convolutional_classifier import Classifier as ConvolutionalClassifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define classifier training function\n",
    "def train_classifier(num_epochs, config, data_dir=None, model_type='convolutional', ckpt=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    parser.add_argument('--data_path', type=str, default='/home/wilson/data/datasets/bair.hdf5')\n",
    "    parser.add_argument('--sequence_length', type=int, default=16)\n",
    "    parser.add_argument('--resolution', type=int, default=64)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--num_workers', type=int, default=8)\n",
    "\n",
    "    if model_type == \"convolutional\":\n",
    "        parser = ConvolutionalClassifier.add_model_specific_args(parser)\n",
    "    else:\n",
    "        parser = TransformerClassifier.add_model_specific_args(parser)\n",
    "\n",
    "    config_args = []\n",
    "    for key in config.keys():\n",
    "        config_args.append(key)\n",
    "        config_args.append(str(config[key]))\n",
    "    config_args.append(\"--data_path\")\n",
    "    config_args.append(data_dir)\n",
    "    config_args.append(\"--gpus\")\n",
    "    config_args.append(str(1))\n",
    "    args = parser.parse_args(config_args)\n",
    "    args.data_path = data_dir\n",
    "\n",
    "    print(\"Creating data loaders ...\")\n",
    "    data = VideoData(args)\n",
    "    # pre-make relevant cached files if necessary\n",
    "    train_loader = data.train_dataloader()\n",
    "    val_loader = data.val_dataloader()\n",
    "    args.n_classes = data.n_classes\n",
    "\n",
    "    if model_type == 'convolutional':\n",
    "        model = ConvolutionalClassifier(args)\n",
    "    else:\n",
    "        model = TransformerClassifier(args)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set training to GPU\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=args.lr)\n",
    "\n",
    "    start_epoch = 0\n",
    "    if ckpt is not None:\n",
    "        checkpoint = torch.load(ckpt)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.train()\n",
    "\n",
    "    print(\"Starting train ...\")\n",
    "    losses = []\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"{epoch=}\")\n",
    "        epoch_loss = 0\n",
    "        epoch_correct = 0\n",
    "        epoch_count = 0\n",
    "        epoch_steps = 0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # Send batch data to the gpu\n",
    "            video, label = batch[\"video\"], batch[\"label\"]\n",
    "            video, label = video.to(device), label.to(device)\n",
    "\n",
    "            predictions = model(video)\n",
    "\n",
    "            loss = criterion(predictions, label)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "\n",
    "            optimizer.step()\n",
    "            losses.append(loss)\n",
    "\n",
    "            correct = predictions.argmax(axis=1) == label\n",
    "            epoch_correct += correct.sum().item()\n",
    "            epoch_count += correct.size(0)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # print statistics\n",
    "            epoch_steps += 1\n",
    "            if epoch_steps % 100 == 99:  # print every 100 mini-batches\n",
    "                print(\n",
    "                    \"[%d, %5d] loss: %.3f\"\n",
    "                    % (epoch + 1, epoch_steps + 1, epoch_loss / epoch_steps)\n",
    "                )\n",
    "                epoch_loss = 0.0\n",
    "\n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_correct = 0\n",
    "        val_epoch_count = 0\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            # Send batch to the gpu\n",
    "            video, label = batch[\"video\"], batch[\"label\"]\n",
    "            video, label = video.to(device), label.to(device)\n",
    "            with torch.no_grad():\n",
    "                predictions = model(video)\n",
    "                val_loss = criterion(predictions, label)\n",
    "                correct = predictions.argmax(axis=1) == label\n",
    "\n",
    "                val_epoch_correct += correct.sum().item()\n",
    "                val_epoch_count += correct.size(0)\n",
    "                val_epoch_loss += val_loss.item()\n",
    "\n",
    "        # Delete previous checkpoint\n",
    "        os.remove(os.path.join(data_dir, f\"checkpoint_{epoch - 1}.ckpt\"))\n",
    "\n",
    "        # Save current checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': val_epoch_loss,\n",
    "        }, os.path.join(data_dir, f\"checkpoint_{epoch}.ckpt\"))\n",
    "\n",
    "        print(f\"{epoch_loss=}\")\n",
    "        print(f\"epoch accuracy: {epoch_correct / epoch_count}\")\n",
    "        print(f\"{val_epoch_loss=}\")\n",
    "        print(f\"test epoch accuracy: {val_epoch_correct / val_epoch_count}\")\n",
    "\n",
    "    return model, optimizer, val_epoch_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set up user-defined variables\n",
    "data_folder = \"/content/drive/BME1570/gesture_sorted_data_loso\"\n",
    "vqvae_path = \"/content/drive/BME1570/vqvae/\"\n",
    "model_type = \"convolutional\"\n",
    "num_epochs = 100\n",
    "checkpoint = None\n",
    "\n",
    "# Config dict for convolutional classifier\n",
    "config = {\n",
    "    \"--batch_size\": 16,\n",
    "    \"--vqvae\": vqvae_path,\n",
    "    \"--kernel_size\": 3,\n",
    "    \"--out_channels\": 3,\n",
    "    \"--n_classes\": 8,\n",
    "    \"--lr\": 8e-4\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run the training loop\n",
    "torch.manual_seed(1234)\n",
    "final_model, optimizer, final_loss = train_classifier(num_epochs, config, data_folder, model_type, checkpoint)\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': final_loss,\n",
    "}, os.path.join(data_folder, f\"final_model.ckpt\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
